# Inherent-Bias
 
The latent space of a neural network has become commonly used as a feature space for data analytics and predictive modeling. The validity of such practices is questioned, as the latent space is inherently biased and unreliable to use as a feature space. These biases are investigated by creating synthetic datasets and conducting statistical tests on the latent space to analyze the presence systematically. The results will reveal that these biases are not only present, but inherent to the latent space â€“ being observed even in unsupervised scenarios. From these results, it can be determined that using the latent space as a feature space will misrepresent the original data resulting in misleading measurements in regard to representation fairness. This work highlights the risks associated with feature extraction from the latent space and encourages a wider discussion of their role in modern machine learning architectures.
